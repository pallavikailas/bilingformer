{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport wandb\nwandb.login() \nwandb.init(project='hinenglish', entity='pallavikailas')\n\nwith open('/kaggle/input/task-1-semeval/MaSaC_train_erc.json', 'r') as file:\n    data = json.load(file)\n\nfor batch in data:\n    # Now you can extract information from each item if it is a dictionary\n    episode = batch.get('episode', None)\n    speakers = batch.get('speakers', None)\n    utterances = batch.get('utterances', None)\n    emotions = batch.get('emotions', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-08T17:06:46.988138Z","iopub.execute_input":"2023-11-08T17:06:46.988424Z","iopub.status.idle":"2023-11-08T17:08:10.918479Z","shell.execute_reply.started":"2023-11-08T17:06:46.988397Z","shell.execute_reply":"2023-11-08T17:08:10.917585Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpallavikailas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231108_170739-9i4spdbm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/pallavikailas/hinenglish/runs/9i4spdbm' target=\"_blank\">lemon-thunder-29</a></strong> to <a href='https://wandb.ai/pallavikailas/hinenglish' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/pallavikailas/hinenglish' target=\"_blank\">https://wandb.ai/pallavikailas/hinenglish</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/pallavikailas/hinenglish/runs/9i4spdbm' target=\"_blank\">https://wandb.ai/pallavikailas/hinenglish/runs/9i4spdbm</a>"},"metadata":{}}]},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\nfrom transformers import BertTokenizer\n\n# Load the JSON data\nwith open('/kaggle/input/task-1-semeval/MaSaC_train_erc.json', 'r') as file:\n    data = json.load(file)\n\n# Flatten the JSON data into a list of sentence and sentiment pairs\n# This part depends on your JSON structure\nutterances = []\nemotions = []\nfor item in data:\n    # Assuming each item has 'utterances' and 'emotions' as lists of same length\n    utterances.extend(item['utterances'])\n    emotions.extend(item['emotions'])\n\n# Define a mapping from emotions to integers\nemotion_to_label = {\n    'neutral': 0,\n    'joy': 1,\n    'contempt': 2,\n    'anger': 3,\n    'surprise': 4,\n    'fear': 5,\n    'disgust': 6,\n    'sadness': 7\n    # Add all your unique emotions and corresponding integers\n}\n\n# Apply the mapping to your emotions data\nlabels = [emotion_to_label[emotion] for emotion in emotions]\n\n# Ensure that `utterances` and `labels` are lists that will form the columns of your dataset\ndataset_dict = {\n    'utterances': utterances,\n    'labels': labels\n}\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_dict(dataset_dict)\n\n# Tokenize the dataset\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_and_format(examples):\n    tokenized_inputs = tokenizer(\n        examples['utterances'],\n        padding='max_length',\n        truncation=True,\n        max_length=128  # Adjust max_length according to your data if needed\n    )\n    tokenized_inputs['labels'] = examples['labels']\n    return tokenized_inputs\n\n# Tokenize and format the dataset\ntokenized_datasets = dataset.map(tokenize_and_format, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T17:08:10.920649Z","iopub.execute_input":"2023-11-08T17:08:10.921532Z","iopub.status.idle":"2023-11-08T17:08:21.056445Z","shell.execute_reply.started":"2023-11-08T17:08:10.921483Z","shell.execute_reply":"2023-11-08T17:08:21.055470Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d880552b757746c9b92a29579be964c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd59829a78cd4a5eaad1a77f815d826d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8691d6d8f1a74d0d8eb6b12d62a4b480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16fa14673527462fa743438965235ce2"}},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport numpy as np\n\ndef calculate_accuracy(preds, labels):\n    # preds could be logits or probabilities, depending on the model output\n    # If preds are logits, convert to probabilities using softmax\n    if preds.ndim > 1 and preds.shape[1] > 1:  # We have logits for more than one class\n        preds = np.argmax(preds, axis=1)\n    elif preds.ndim > 1 and preds.shape[1] == 1:  # We have one logit\n        preds = np.squeeze((preds > 0).astype(int), axis=1)\n    \n    # Calculate the accuracy comparing against the true labels\n    return accuracy_score(labels, preds)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T17:08:21.057786Z","iopub.execute_input":"2023-11-08T17:08:21.058238Z","iopub.status.idle":"2023-11-08T17:08:22.638181Z","shell.execute_reply.started":"2023-11-08T17:08:21.058210Z","shell.execute_reply":"2023-11-08T17:08:22.637247Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset, val_dataset = tokenized_datasets.train_test_split(test_size=0.1).values()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T17:08:22.642411Z","iopub.execute_input":"2023-11-08T17:08:22.642710Z","iopub.status.idle":"2023-11-08T17:08:22.661877Z","shell.execute_reply.started":"2023-11-08T17:08:22.642683Z","shell.execute_reply":"2023-11-08T17:08:22.660920Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\n# Set the number of labels\nnum_labels = len(emotion_to_label)  # This should be 8 in your case\n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=num_labels  # Specify the number of labels here\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T17:08:22.663011Z","iopub.execute_input":"2023-11-08T17:08:22.663348Z","iopub.status.idle":"2023-11-08T17:08:45.151452Z","shell.execute_reply.started":"2023-11-08T17:08:22.663316Z","shell.execute_reply":"2023-11-08T17:08:45.150143Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36d57b6f546b46309e7299daee1d7a1a"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, TrainerCallback, EvalPrediction\nfrom pytorch_lightning.callbacks import Callback\nimport shutil\nimport os\nimport warnings\nimport torch\nfrom torch import nn\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.parallel._functions\")\n\n\ndef compute_metrics(p: EvalPrediction):\n    # Extract the predictions and labels from EvalPrediction object\n    preds = p.predictions\n    labels = p.label_ids\n    # If preds are logits, convert to probabilities using softmax\n    if preds.ndim > 1 and preds.shape[1] > 1:  # We have logits for more than one class\n        preds = np.argmax(preds, axis=1)\n    elif preds.ndim > 1 and preds.shape[1] == 1:  # We have one logit\n        preds = np.squeeze((preds > 0).astype(int), axis=1)\n    \n    # Calculate the accuracy comparing against the true labels\n    accuracy = accuracy_score(labels, preds)\n    return {\"accuracy\": accuracy}\n\n\n# Modify the WandbValidationCallback to include accuracy\nclass WandbValidationCallback(TrainerCallback):\n    \"\"\"\n    A custom callback that logs validation loss, metrics, and custom fine-tuning metrics to wandb.\n    \"\"\"\n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        # Log validation loss with the key 'loss' and any additional metrics\n        if metrics is not None:\n            wandb.log({\"loss\": metrics['eval_loss'], \"accuracy\": metrics.get('accuracy', 0)}, step=state.global_step)   \n\nclass SaveBestModelCallback(TrainerCallback):\n    \"\"\"\n    A custom callback that saves only the best model's state_dict at the end of training.\n    \"\"\"\n    def on_train_end(self, args, state, control, **kwargs):\n        # Assuming the best model is loaded at the end of training\n        if state.is_local_process_zero:\n            # Save the best model's state_dict\n            torch.save(model.state_dict(), os.path.join(args.output_dir, 'pytorch_model.bin'))\n\n            # Clean up all other checkpoints\n            checkpoints = [os.path.join(args.output_dir, name) for name in os.listdir(args.output_dir) if name.startswith(\"checkpoint\")]\n            for checkpoint in checkpoints:\n                shutil.rmtree(checkpoint)\n            \n            \ntraining_args = TrainingArguments(\n    output_dir='./results',\n    #save_total_limit=1,\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"steps\",\n    eval_steps=10,\n    save_strategy=\"steps\",\n    save_steps=10,\n    load_best_model_at_end=True,  # Moved here\n    metric_for_best_model=\"accuracy\",  # Moved here\n    greater_is_better=True,  # Moved here\n    report_to=\"wandb\",\n    run_name=\"run1\"\n)\n\n# Initialize Trainer with a model and datasets\ntrainer = Trainer(\n    model=model,  # Make sure your model is defined or loaded\n    args=training_args,\n    train_dataset=train_dataset,  # Make sure your training dataset is defined\n    eval_dataset=val_dataset, # Make sure your validation dataset is defined\n    compute_metrics=compute_metrics,  # Make sure your compute_metrics function is defined\n    callbacks=[WandbValidationCallback(), SaveBestModelCallback()]\n)\n\n# Start training\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T17:08:45.153255Z","iopub.execute_input":"2023-11-08T17:08:45.154053Z","iopub.status.idle":"2023-11-08T17:53:33.204169Z","shell.execute_reply.started":"2023-11-08T17:08:45.154012Z","shell.execute_reply":"2023-11-08T17:53:33.203166Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2400' max='2400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2400/2400 44:30, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.200700</td>\n      <td>2.184168</td>\n      <td>0.059929</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.136500</td>\n      <td>2.073740</td>\n      <td>0.085781</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.048400</td>\n      <td>1.978463</td>\n      <td>0.309048</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.967300</td>\n      <td>1.889552</td>\n      <td>0.444183</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.854700</td>\n      <td>1.829509</td>\n      <td>0.477086</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.849700</td>\n      <td>1.773251</td>\n      <td>0.480611</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.734300</td>\n      <td>1.711656</td>\n      <td>0.480611</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.729100</td>\n      <td>1.668067</td>\n      <td>0.481786</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.648400</td>\n      <td>1.641875</td>\n      <td>0.482961</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.684700</td>\n      <td>1.631017</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.687300</td>\n      <td>1.608274</td>\n      <td>0.490012</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.671600</td>\n      <td>1.591987</td>\n      <td>0.490012</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.536300</td>\n      <td>1.577320</td>\n      <td>0.485311</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.612500</td>\n      <td>1.574464</td>\n      <td>0.482961</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.449100</td>\n      <td>1.566905</td>\n      <td>0.487662</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.644200</td>\n      <td>1.550426</td>\n      <td>0.499412</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.552600</td>\n      <td>1.558599</td>\n      <td>0.495887</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.612000</td>\n      <td>1.535385</td>\n      <td>0.500588</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.571600</td>\n      <td>1.531973</td>\n      <td>0.493537</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.646200</td>\n      <td>1.531958</td>\n      <td>0.499412</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.652200</td>\n      <td>1.545239</td>\n      <td>0.509988</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.637400</td>\n      <td>1.523687</td>\n      <td>0.504113</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.529100</td>\n      <td>1.531123</td>\n      <td>0.505288</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.666500</td>\n      <td>1.553084</td>\n      <td>0.504113</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.660100</td>\n      <td>1.532486</td>\n      <td>0.508813</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.612400</td>\n      <td>1.513217</td>\n      <td>0.513514</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.555600</td>\n      <td>1.512826</td>\n      <td>0.504113</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.575200</td>\n      <td>1.497593</td>\n      <td>0.517039</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.599800</td>\n      <td>1.499770</td>\n      <td>0.515864</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.511700</td>\n      <td>1.478513</td>\n      <td>0.518214</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.509400</td>\n      <td>1.474001</td>\n      <td>0.524089</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.535000</td>\n      <td>1.475500</td>\n      <td>0.521739</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.553500</td>\n      <td>1.478188</td>\n      <td>0.521739</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.410800</td>\n      <td>1.469010</td>\n      <td>0.515864</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.493100</td>\n      <td>1.462627</td>\n      <td>0.521739</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.557000</td>\n      <td>1.469149</td>\n      <td>0.506463</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.509000</td>\n      <td>1.478534</td>\n      <td>0.505288</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.465800</td>\n      <td>1.448627</td>\n      <td>0.513514</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.517400</td>\n      <td>1.469517</td>\n      <td>0.512338</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.545000</td>\n      <td>1.522055</td>\n      <td>0.488837</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.572600</td>\n      <td>1.498335</td>\n      <td>0.508813</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.456300</td>\n      <td>1.486545</td>\n      <td>0.511163</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.591100</td>\n      <td>1.486840</td>\n      <td>0.515864</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.579700</td>\n      <td>1.460792</td>\n      <td>0.519389</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.533400</td>\n      <td>1.465008</td>\n      <td>0.518214</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.558700</td>\n      <td>1.458266</td>\n      <td>0.520564</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.472500</td>\n      <td>1.450819</td>\n      <td>0.518214</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.558500</td>\n      <td>1.442011</td>\n      <td>0.504113</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.571400</td>\n      <td>1.465926</td>\n      <td>0.504113</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.388600</td>\n      <td>1.456318</td>\n      <td>0.512338</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.508300</td>\n      <td>1.473374</td>\n      <td>0.520564</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.342300</td>\n      <td>1.468973</td>\n      <td>0.517039</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.458700</td>\n      <td>1.467165</td>\n      <td>0.498237</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.421200</td>\n      <td>1.450643</td>\n      <td>0.511163</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.456800</td>\n      <td>1.447064</td>\n      <td>0.508813</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.469500</td>\n      <td>1.430779</td>\n      <td>0.509988</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>1.394100</td>\n      <td>1.433017</td>\n      <td>0.529965</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.358100</td>\n      <td>1.444649</td>\n      <td>0.525264</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>1.418400</td>\n      <td>1.403065</td>\n      <td>0.534665</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.476000</td>\n      <td>1.420560</td>\n      <td>0.513514</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>1.316900</td>\n      <td>1.442725</td>\n      <td>0.495887</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.421300</td>\n      <td>1.415460</td>\n      <td>0.514689</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>1.397900</td>\n      <td>1.414252</td>\n      <td>0.519389</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.408000</td>\n      <td>1.417537</td>\n      <td>0.519389</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.434900</td>\n      <td>1.434353</td>\n      <td>0.511163</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.485800</td>\n      <td>1.438173</td>\n      <td>0.519389</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>1.471500</td>\n      <td>1.459473</td>\n      <td>0.518214</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>1.460800</td>\n      <td>1.432173</td>\n      <td>0.505288</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>1.489100</td>\n      <td>1.426032</td>\n      <td>0.527615</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.407400</td>\n      <td>1.473749</td>\n      <td>0.495887</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>1.335000</td>\n      <td>1.404616</td>\n      <td>0.528790</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.411900</td>\n      <td>1.461770</td>\n      <td>0.500588</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>1.274900</td>\n      <td>1.394346</td>\n      <td>0.511163</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.111600</td>\n      <td>1.464770</td>\n      <td>0.514689</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.203100</td>\n      <td>1.471366</td>\n      <td>0.514689</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.207900</td>\n      <td>1.514034</td>\n      <td>0.473561</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>1.286800</td>\n      <td>1.555897</td>\n      <td>0.455934</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.302000</td>\n      <td>1.459661</td>\n      <td>0.492362</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>1.284100</td>\n      <td>1.429587</td>\n      <td>0.495887</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.168900</td>\n      <td>1.434044</td>\n      <td>0.514689</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>1.157100</td>\n      <td>1.432621</td>\n      <td>0.514689</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.231300</td>\n      <td>1.404327</td>\n      <td>0.527615</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>1.170900</td>\n      <td>1.411400</td>\n      <td>0.522914</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>1.199200</td>\n      <td>1.497494</td>\n      <td>0.504113</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.262300</td>\n      <td>1.481142</td>\n      <td>0.501763</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>1.236300</td>\n      <td>1.438978</td>\n      <td>0.515864</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>1.138900</td>\n      <td>1.440818</td>\n      <td>0.519389</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>1.147500</td>\n      <td>1.484402</td>\n      <td>0.495887</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>1.180400</td>\n      <td>1.417526</td>\n      <td>0.515864</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.138300</td>\n      <td>1.422867</td>\n      <td>0.527615</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>1.205600</td>\n      <td>1.472058</td>\n      <td>0.482961</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>1.207400</td>\n      <td>1.426173</td>\n      <td>0.521739</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>1.097400</td>\n      <td>1.407838</td>\n      <td>0.525264</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>1.140700</td>\n      <td>1.436061</td>\n      <td>0.513514</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.205400</td>\n      <td>1.422629</td>\n      <td>0.497062</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>1.146300</td>\n      <td>1.408225</td>\n      <td>0.526439</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.874700</td>\n      <td>1.455441</td>\n      <td>0.511163</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.919200</td>\n      <td>1.555980</td>\n      <td>0.532315</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.937300</td>\n      <td>1.502058</td>\n      <td>0.512338</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.828100</td>\n      <td>1.510104</td>\n      <td>0.511163</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.820400</td>\n      <td>1.576554</td>\n      <td>0.478261</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.932400</td>\n      <td>1.557454</td>\n      <td>0.519389</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.927900</td>\n      <td>1.529375</td>\n      <td>0.514689</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.820800</td>\n      <td>1.477132</td>\n      <td>0.524089</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.892300</td>\n      <td>1.567812</td>\n      <td>0.492362</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.962900</td>\n      <td>1.515511</td>\n      <td>0.514689</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>1.013000</td>\n      <td>1.673448</td>\n      <td>0.448884</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.919200</td>\n      <td>1.534189</td>\n      <td>0.509988</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.988000</td>\n      <td>1.504938</td>\n      <td>0.520564</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.988300</td>\n      <td>1.538451</td>\n      <td>0.506463</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.834900</td>\n      <td>1.574084</td>\n      <td>0.528790</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.958600</td>\n      <td>1.600269</td>\n      <td>0.491187</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.873300</td>\n      <td>1.611122</td>\n      <td>0.492362</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.915200</td>\n      <td>1.581283</td>\n      <td>0.499412</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.902200</td>\n      <td>1.540935</td>\n      <td>0.518214</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.900300</td>\n      <td>1.709969</td>\n      <td>0.438308</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.910300</td>\n      <td>1.572973</td>\n      <td>0.508813</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.913400</td>\n      <td>1.561509</td>\n      <td>0.485311</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.824000</td>\n      <td>1.547916</td>\n      <td>0.495887</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.758700</td>\n      <td>1.578979</td>\n      <td>0.517039</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.556000</td>\n      <td>1.714971</td>\n      <td>0.464160</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.599600</td>\n      <td>1.690745</td>\n      <td>0.508813</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.658600</td>\n      <td>1.710330</td>\n      <td>0.491187</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.637800</td>\n      <td>1.735689</td>\n      <td>0.462985</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.579000</td>\n      <td>1.810084</td>\n      <td>0.515864</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.704300</td>\n      <td>1.853819</td>\n      <td>0.445358</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.678000</td>\n      <td>1.668686</td>\n      <td>0.494712</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.553400</td>\n      <td>1.745624</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.598800</td>\n      <td>1.739846</td>\n      <td>0.478261</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.620900</td>\n      <td>1.726405</td>\n      <td>0.500588</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.729000</td>\n      <td>1.735077</td>\n      <td>0.500588</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.585300</td>\n      <td>1.724648</td>\n      <td>0.493537</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.661600</td>\n      <td>1.746619</td>\n      <td>0.501763</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.509200</td>\n      <td>1.798120</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.617700</td>\n      <td>1.857414</td>\n      <td>0.466510</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.599000</td>\n      <td>1.830923</td>\n      <td>0.495887</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.617400</td>\n      <td>1.805212</td>\n      <td>0.478261</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.651300</td>\n      <td>1.762350</td>\n      <td>0.511163</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.565200</td>\n      <td>1.777932</td>\n      <td>0.461810</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.612400</td>\n      <td>1.819551</td>\n      <td>0.517039</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.701300</td>\n      <td>1.831895</td>\n      <td>0.467685</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.752800</td>\n      <td>1.793570</td>\n      <td>0.499412</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.729900</td>\n      <td>1.842823</td>\n      <td>0.443008</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.644600</td>\n      <td>1.727520</td>\n      <td>0.500588</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.397900</td>\n      <td>1.864996</td>\n      <td>0.475911</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.432200</td>\n      <td>1.924629</td>\n      <td>0.493537</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.363500</td>\n      <td>1.987513</td>\n      <td>0.461810</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.432800</td>\n      <td>1.978410</td>\n      <td>0.451234</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.380600</td>\n      <td>1.978356</td>\n      <td>0.497062</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.458000</td>\n      <td>1.919132</td>\n      <td>0.445358</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.384900</td>\n      <td>1.916215</td>\n      <td>0.484136</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.375200</td>\n      <td>2.003465</td>\n      <td>0.475911</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.484900</td>\n      <td>1.967653</td>\n      <td>0.465335</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.421400</td>\n      <td>1.953447</td>\n      <td>0.465335</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.540300</td>\n      <td>1.952535</td>\n      <td>0.481786</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.405800</td>\n      <td>1.951143</td>\n      <td>0.472385</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.425900</td>\n      <td>1.988362</td>\n      <td>0.431257</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.392000</td>\n      <td>1.906075</td>\n      <td>0.498237</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.460900</td>\n      <td>2.005573</td>\n      <td>0.452409</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.356400</td>\n      <td>1.953387</td>\n      <td>0.478261</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.341800</td>\n      <td>2.008603</td>\n      <td>0.481786</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.411900</td>\n      <td>2.020028</td>\n      <td>0.467685</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.437100</td>\n      <td>1.979609</td>\n      <td>0.478261</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.394400</td>\n      <td>2.034541</td>\n      <td>0.473561</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.413000</td>\n      <td>1.999582</td>\n      <td>0.477086</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.413800</td>\n      <td>2.098005</td>\n      <td>0.454759</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.434400</td>\n      <td>1.962223</td>\n      <td>0.506463</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.406000</td>\n      <td>1.961037</td>\n      <td>0.493537</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.325700</td>\n      <td>2.034377</td>\n      <td>0.474736</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.306900</td>\n      <td>2.057664</td>\n      <td>0.460635</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.246200</td>\n      <td>2.097164</td>\n      <td>0.437133</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.277800</td>\n      <td>2.106029</td>\n      <td>0.468860</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.314400</td>\n      <td>2.121813</td>\n      <td>0.454759</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.267200</td>\n      <td>2.151375</td>\n      <td>0.452409</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.242400</td>\n      <td>2.135621</td>\n      <td>0.473561</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.296400</td>\n      <td>2.142312</td>\n      <td>0.477086</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.321600</td>\n      <td>2.196778</td>\n      <td>0.454759</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.300000</td>\n      <td>2.195378</td>\n      <td>0.455934</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.306500</td>\n      <td>2.126321</td>\n      <td>0.494712</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.319300</td>\n      <td>2.121974</td>\n      <td>0.491187</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.310400</td>\n      <td>2.178270</td>\n      <td>0.455934</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.333400</td>\n      <td>2.168328</td>\n      <td>0.474736</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.163900</td>\n      <td>2.158526</td>\n      <td>0.470035</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.222400</td>\n      <td>2.221658</td>\n      <td>0.467685</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.312100</td>\n      <td>2.208014</td>\n      <td>0.494712</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.288100</td>\n      <td>2.241002</td>\n      <td>0.464160</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.284100</td>\n      <td>2.209690</td>\n      <td>0.493537</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.213800</td>\n      <td>2.217399</td>\n      <td>0.497062</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.327200</td>\n      <td>2.224844</td>\n      <td>0.471210</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.319900</td>\n      <td>2.237950</td>\n      <td>0.471210</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.347200</td>\n      <td>2.206431</td>\n      <td>0.484136</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.256500</td>\n      <td>2.237951</td>\n      <td>0.464160</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.184900</td>\n      <td>2.180480</td>\n      <td>0.480611</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.210500</td>\n      <td>2.205651</td>\n      <td>0.481786</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.218800</td>\n      <td>2.216534</td>\n      <td>0.480611</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.179600</td>\n      <td>2.239602</td>\n      <td>0.495887</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.208300</td>\n      <td>2.306668</td>\n      <td>0.460635</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.177100</td>\n      <td>2.260320</td>\n      <td>0.498237</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.189500</td>\n      <td>2.280758</td>\n      <td>0.493537</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.145300</td>\n      <td>2.295978</td>\n      <td>0.468860</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.214100</td>\n      <td>2.337776</td>\n      <td>0.457109</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.206400</td>\n      <td>2.344973</td>\n      <td>0.474736</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.252400</td>\n      <td>2.366035</td>\n      <td>0.470035</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.200500</td>\n      <td>2.292394</td>\n      <td>0.485311</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.265200</td>\n      <td>2.279312</td>\n      <td>0.495887</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.177800</td>\n      <td>2.294324</td>\n      <td>0.465335</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.247400</td>\n      <td>2.266775</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.219400</td>\n      <td>2.271119</td>\n      <td>0.484136</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.213800</td>\n      <td>2.267778</td>\n      <td>0.475911</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.209100</td>\n      <td>2.274671</td>\n      <td>0.467685</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.250400</td>\n      <td>2.271269</td>\n      <td>0.467685</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.166600</td>\n      <td>2.295794</td>\n      <td>0.460635</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.193700</td>\n      <td>2.301288</td>\n      <td>0.477086</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.192100</td>\n      <td>2.292786</td>\n      <td>0.481786</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.245800</td>\n      <td>2.289797</td>\n      <td>0.490012</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.212600</td>\n      <td>2.296434</td>\n      <td>0.465335</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.139800</td>\n      <td>2.308280</td>\n      <td>0.467685</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.198500</td>\n      <td>2.317293</td>\n      <td>0.487662</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.106800</td>\n      <td>2.349437</td>\n      <td>0.471210</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.141500</td>\n      <td>2.364598</td>\n      <td>0.466510</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.154100</td>\n      <td>2.337952</td>\n      <td>0.472385</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.164500</td>\n      <td>2.337607</td>\n      <td>0.487662</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.171400</td>\n      <td>2.345135</td>\n      <td>0.473561</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.225600</td>\n      <td>2.359753</td>\n      <td>0.478261</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.195900</td>\n      <td>2.378737</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.144200</td>\n      <td>2.381714</td>\n      <td>0.478261</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.135400</td>\n      <td>2.369777</td>\n      <td>0.477086</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.164100</td>\n      <td>2.359063</td>\n      <td>0.471210</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.148700</td>\n      <td>2.361628</td>\n      <td>0.471210</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.112400</td>\n      <td>2.366841</td>\n      <td>0.473561</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.130500</td>\n      <td>2.366549</td>\n      <td>0.474736</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.130000</td>\n      <td>2.370910</td>\n      <td>0.473561</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.097200</td>\n      <td>2.373778</td>\n      <td>0.473561</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.165300</td>\n      <td>2.373621</td>\n      <td>0.473561</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.170500</td>\n      <td>2.377369</td>\n      <td>0.468860</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.183700</td>\n      <td>2.384804</td>\n      <td>0.471210</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.139300</td>\n      <td>2.384033</td>\n      <td>0.468860</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.132500</td>\n      <td>2.384563</td>\n      <td>0.470035</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.154000</td>\n      <td>2.384563</td>\n      <td>0.471210</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.244600</td>\n      <td>2.384187</td>\n      <td>0.468860</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2400, training_loss=0.8485129996885856, metrics={'train_runtime': 2676.4372, 'train_samples_per_second': 28.601, 'train_steps_per_second': 0.897, 'total_flos': 5035559080857600.0, 'train_loss': 0.8485129996885856, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=val_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T17:53:33.205480Z","iopub.execute_input":"2023-11-08T17:53:33.205809Z","iopub.status.idle":"2023-11-08T17:53:36.957034Z","shell.execute_reply.started":"2023-11-08T17:53:33.205783Z","shell.execute_reply":"2023-11-08T17:53:36.955725Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7/7 00:03]\n    </div>\n    "},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 1.4030647277832031,\n 'eval_accuracy': 0.5346650998824912,\n 'eval_runtime': 3.7396,\n 'eval_samples_per_second': 227.564,\n 'eval_steps_per_second': 1.872,\n 'epoch': 10.0}"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained('./results')\ntokenizer.save_pretrained('./results')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T17:53:36.958636Z","iopub.execute_input":"2023-11-08T17:53:36.959537Z","iopub.status.idle":"2023-11-08T17:53:37.855027Z","shell.execute_reply.started":"2023-11-08T17:53:36.959495Z","shell.execute_reply":"2023-11-08T17:53:37.853957Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('./results/tokenizer_config.json',\n './results/special_tokens_map.json',\n './results/vocab.txt',\n './results/added_tokens.json')"},"metadata":{}}]}]}